Title,Link,Body
Nuance partners with The Academy to launch The AI Collaborative,https://www.artificialintelligence-news.com/2022/05/13/nuance-partners-the-academy-launch-the-ai-collaborative/,"  Nuance has partnered with The Health Management Academy (The Academy) to launch The AI Collaborative, an industry group focused on advancing healthcare using artificial intelligence and machine learning. Nuance became a household name for creating the speech engine recognition engine behind Siri. In recent years, the company has put a strong focus on AI solutions for healthcare and is now a full-service partner of 77 percent of US hospitals and is trusted by over 500,000 physicians daily. Earlier this year, Microsoft acquired Nuance with the promise of ushering in a “new era of outcomes-based AI”. Microsoft is also active in the healthcare space and its acquisition of Nuance was investigated by regulators over concerns it may reduce competition. Regulators ultimately ended up giving the deal the thumbs up. The EU’s regulator, for example, concluded that Nuance would continue to face stiff competition in the future, the data Microsoft gains would not provide it with an advantage to shut out competitors, and there’d be no ability/incentive to foreclose existing solutions. “Combining the power of Nuance’s deep vertical expertise and proven business outcomes across healthcare, financial services, retail, telecommunications, and other industries with Microsoft’s global cloud ecosystems will enable us to accelerate our innovation and deploy our solutions more quickly, more seamlessly, and at greater scale to solve our customers’ most pressing challenges,” said Mark Benjamin, CEO of Nuance, at the time of Microsoft’s acquisition. Nuance says both companies represent two of the most trusted and innovative technology organisations in the world. As such, it believes Nuance and Microsoft are in a position to foster a community anchored in collaboration with key leading health systems (LHS) executives and experts across the healthcare ecosystem. That community will be The AI Collaborative. “Our members have expressed their desire for a dedicated space to explore AI in healthcare and its enormous potential to improve outcomes and clinical workflow,” said Renee DeSilva, CEO of The Academy. “We are thrilled to expand our partnership with Microsoft and Nuance to introduce The AI Collaborative, a new program at The Academy designed exclusively for clinical and operational executives who lead their organization’s approach to investing in AI as a strategic initiative.” The AI Collaborative will bring together senior leaders from LHS to understand their current and future needs and create the AI and ML innovations required to fulfil them. “The key to successful healthcare innovation using AI is understanding at a deep level the problems that you’re trying to solve and focusing on the outcomes you want to achieve,” explained Peter Durlach, Chief Strategy Officer of Nuance. “With the combined engineering, market and domain expertise of Nuance and Microsoft, The AI Collaborative can bring together multiple technical, business and clinical stakeholders to prioritize deployment of solutions for clinician burnout, patient engagement and health system financial stability, while accelerating innovation in precision medicine, drug discovery, clinical decision support and other promising use cases across the entire healthcare ecosystem.” The AI Collaborative will commence in September 2022 and kick off with a visit to Microsoft’s corporate HQ. Annual summits will be held going forward where stakeholders will learn how to best utilise patient-specific data and insights to augment care delivery, reduce care variation, and support operational improvements. (Photo by Cytonn Photography on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here."
Nuance partners with The Academy to launch The AI Collaborative,https://www.artificialintelligence-news.com/2022/05/13/nuance-partners-the-academy-launch-the-ai-collaborative/,"  Nuance has partnered with The Health Management Academy (The Academy) to launch The AI Collaborative, an industry group focused on advancing healthcare using artificial intelligence and machine learning. Nuance became a household name for creating the speech engine recognition engine behind Siri. In recent years, the company has put a strong focus on AI solutions for healthcare and is now a full-service partner of 77 percent of US hospitals and is trusted by over 500,000 physicians daily. Earlier this year, Microsoft acquired Nuance with the promise of ushering in a “new era of outcomes-based AI”. Microsoft is also active in the healthcare space and its acquisition of Nuance was investigated by regulators over concerns it may reduce competition. Regulators ultimately ended up giving the deal the thumbs up. The EU’s regulator, for example, concluded that Nuance would continue to face stiff competition in the future, the data Microsoft gains would not provide it with an advantage to shut out competitors, and there’d be no ability/incentive to foreclose existing solutions. “Combining the power of Nuance’s deep vertical expertise and proven business outcomes across healthcare, financial services, retail, telecommunications, and other industries with Microsoft’s global cloud ecosystems will enable us to accelerate our innovation and deploy our solutions more quickly, more seamlessly, and at greater scale to solve our customers’ most pressing challenges,” said Mark Benjamin, CEO of Nuance, at the time of Microsoft’s acquisition. Nuance says both companies represent two of the most trusted and innovative technology organisations in the world. As such, it believes Nuance and Microsoft are in a position to foster a community anchored in collaboration with key leading health systems (LHS) executives and experts across the healthcare ecosystem. That community will be The AI Collaborative. “Our members have expressed their desire for a dedicated space to explore AI in healthcare and its enormous potential to improve outcomes and clinical workflow,” said Renee DeSilva, CEO of The Academy. “We are thrilled to expand our partnership with Microsoft and Nuance to introduce The AI Collaborative, a new program at The Academy designed exclusively for clinical and operational executives who lead their organization’s approach to investing in AI as a strategic initiative.” The AI Collaborative will bring together senior leaders from LHS to understand their current and future needs and create the AI and ML innovations required to fulfil them. “The key to successful healthcare innovation using AI is understanding at a deep level the problems that you’re trying to solve and focusing on the outcomes you want to achieve,” explained Peter Durlach, Chief Strategy Officer of Nuance. “With the combined engineering, market and domain expertise of Nuance and Microsoft, The AI Collaborative can bring together multiple technical, business and clinical stakeholders to prioritize deployment of solutions for clinician burnout, patient engagement and health system financial stability, while accelerating innovation in precision medicine, drug discovery, clinical decision support and other promising use cases across the entire healthcare ecosystem.” The AI Collaborative will commence in September 2022 and kick off with a visit to Microsoft’s corporate HQ. Annual summits will be held going forward where stakeholders will learn how to best utilise patient-specific data and insights to augment care delivery, reduce care variation, and support operational improvements. (Photo by Cytonn Photography on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here."
Clearview AI agrees to restrict sales of its faceprint database,https://www.artificialintelligence-news.com/2022/05/10/clearview-ai-agrees-restrict-sales-faceprint-database/,"  Clearview AI has proposed to restrict sales of its faceprint database as part of a settlement with the American Civil Liberties Union (ACLU). The controversial facial recognition firm caused a stir due to scraping billions of images of people across the web without their consent. As a result, the company has faced the ire of regulators around the world and numerous court cases. One court case filed against Clearview AI was by the ACLU in 2020, claiming that it violated the Biometric Information Privacy Act (BIPA). That act covers Illinois and requires companies operating in the state to obtain explicit consent from individuals to collect their biometric data. “Fourteen years ago, the ACLU of Illinois led the effort to enact BIPA – a groundbreaking statute to deal with the growing use of sensitive biometric information without any notice and without meaningful consent,” explained Rebecca Glenberg, staff attorney for the ACLU of Illinois. “BIPA was intended to curb exactly the kind of broad-based surveillance that Clearview’s app enables.” The case is ongoing but the two sides have reached a draft settlement. As part of the proposal, Clearview AI has agreed to restrict sales of its faceprint database to businesses and other private entities across the country. “By requiring Clearview to comply with Illinois’ pathbreaking biometric privacy law not just in the state, but across the country, this settlement demonstrates that strong privacy laws can provide real protections against abuse,” said Nathan Freed Wessler, a deputy director of the ACLU Speech, Privacy, and Technology Project. “Clearview can no longer treat people’s unique biometric identifiers as an unrestricted source of profit. Other companies would be wise to take note, and other states should follow Illinois’ lead in enacting strong biometric privacy laws.”  The most protections will be offered to residents in Illinois. Clearview AI will be banned from sharing access to its database to any private company in the state in addition to any local public entity for five years. Furthermore, Clearview AI plans to filter out images from Illinois. This may not catch all images so residents will be able to upload their image and Clearview will block its software from finding matches for their face. Clearview AI will spend $50,000 on adverts in online ads to raise awareness for this feature. “This settlement is a big win for the most vulnerable people in Illinois,” commented Linda Xóchitl Tortolero, president and CEO of Mujeres Latinas en Acción, a Chicago-based non-profit. “Much of our work centres on protecting privacy and ensuring the safety of survivors of domestic violence and sexual assault. Before this agreement, Clearview ignored the fact that biometric information can be misused to create dangerous situations and threats to their lives. Today that’s no longer the case.”  The protections offered to American citizens outside Illinois aren’t quite as stringent. Clearview AI is still able to sell access to its huge database to public entities, including law enforcement. In the wake of the US Capitol raid, the company boasted that police use of its facial recognition system increased 26 percent. However, the company would be banned from selling access to its complete database to private companies. Clearview AI could still sell its software, but any purchaser would need to source their own database to train it. “There is a battle being fought in courtrooms and statehouses across the country about who is going to control biometrics—Big Tech or the people being tracked by them—and this represents one of the biggest victories for consumers to date,” said J. Eli Wade-Scott from Edelson PC. In November 2021, the UK’s Information Commissioner’s Office (ICO) imposed a potential fine of just over £17 million to Clearview AI and ordered the company to destroy the personal data it holds on British citizens and cease further processing. Earlier that month, the OAIC reached the same conclusion as the ICO and ordered Clearview AI to destroy the biometric data it collected on Australians and cease further collection. The full draft settlement between Clearview AI and the ACLU can be found here. (Photo by Maksim Chernishev on Unsplash) Related: Ukraine harnesses Clearview AI to uncover assailants and identify the fallen Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: aclu, american civil liberties union, biometric information privacy act, bipa, clearview ai, ethics, face recognition, facial recognition, law, legal, Legislation, privacy, Society, surveillance "
Clearview AI agrees to restrict sales of its faceprint database,https://www.artificialintelligence-news.com/2022/05/10/clearview-ai-agrees-restrict-sales-faceprint-database/,"  Clearview AI has proposed to restrict sales of its faceprint database as part of a settlement with the American Civil Liberties Union (ACLU). The controversial facial recognition firm caused a stir due to scraping billions of images of people across the web without their consent. As a result, the company has faced the ire of regulators around the world and numerous court cases. One court case filed against Clearview AI was by the ACLU in 2020, claiming that it violated the Biometric Information Privacy Act (BIPA). That act covers Illinois and requires companies operating in the state to obtain explicit consent from individuals to collect their biometric data. “Fourteen years ago, the ACLU of Illinois led the effort to enact BIPA – a groundbreaking statute to deal with the growing use of sensitive biometric information without any notice and without meaningful consent,” explained Rebecca Glenberg, staff attorney for the ACLU of Illinois. “BIPA was intended to curb exactly the kind of broad-based surveillance that Clearview’s app enables.” The case is ongoing but the two sides have reached a draft settlement. As part of the proposal, Clearview AI has agreed to restrict sales of its faceprint database to businesses and other private entities across the country. “By requiring Clearview to comply with Illinois’ pathbreaking biometric privacy law not just in the state, but across the country, this settlement demonstrates that strong privacy laws can provide real protections against abuse,” said Nathan Freed Wessler, a deputy director of the ACLU Speech, Privacy, and Technology Project. “Clearview can no longer treat people’s unique biometric identifiers as an unrestricted source of profit. Other companies would be wise to take note, and other states should follow Illinois’ lead in enacting strong biometric privacy laws.”  The most protections will be offered to residents in Illinois. Clearview AI will be banned from sharing access to its database to any private company in the state in addition to any local public entity for five years. Furthermore, Clearview AI plans to filter out images from Illinois. This may not catch all images so residents will be able to upload their image and Clearview will block its software from finding matches for their face. Clearview AI will spend $50,000 on adverts in online ads to raise awareness for this feature. “This settlement is a big win for the most vulnerable people in Illinois,” commented Linda Xóchitl Tortolero, president and CEO of Mujeres Latinas en Acción, a Chicago-based non-profit. “Much of our work centres on protecting privacy and ensuring the safety of survivors of domestic violence and sexual assault. Before this agreement, Clearview ignored the fact that biometric information can be misused to create dangerous situations and threats to their lives. Today that’s no longer the case.”  The protections offered to American citizens outside Illinois aren’t quite as stringent. Clearview AI is still able to sell access to its huge database to public entities, including law enforcement. In the wake of the US Capitol raid, the company boasted that police use of its facial recognition system increased 26 percent. However, the company would be banned from selling access to its complete database to private companies. Clearview AI could still sell its software, but any purchaser would need to source their own database to train it. “There is a battle being fought in courtrooms and statehouses across the country about who is going to control biometrics—Big Tech or the people being tracked by them—and this represents one of the biggest victories for consumers to date,” said J. Eli Wade-Scott from Edelson PC. In November 2021, the UK’s Information Commissioner’s Office (ICO) imposed a potential fine of just over £17 million to Clearview AI and ordered the company to destroy the personal data it holds on British citizens and cease further processing. Earlier that month, the OAIC reached the same conclusion as the ICO and ordered Clearview AI to destroy the biometric data it collected on Australians and cease further collection. The full draft settlement between Clearview AI and the ACLU can be found here. (Photo by Maksim Chernishev on Unsplash) Related: Ukraine harnesses Clearview AI to uncover assailants and identify the fallen Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: aclu, american civil liberties union, biometric information privacy act, bipa, clearview ai, ethics, face recognition, facial recognition, law, legal, Legislation, privacy, Society, surveillance "
What leveraging AI in hybrid security systems means for enterprises,https://www.artificialintelligence-news.com/2022/05/10/what-leveraging-ai-in-hybrid-security-systems-means-for-enterprises/,"  Artificial intelligence (AI) is becoming more common than you may realise. Many of society’s leading technologies are driven by AI technology, as their automated functions streamline processes and help people do more with less time. Now, AI is integrating into commercial security systems and starting to revolutionise technology. Modern security systems with AI technology can help security teams better detect threats and provide faster responses to protect your business more effectively.  Enterprises can leverage AI to enable security operators to analyse data more efficiently and streamline operations, allowing teams to adjust their focuses to more critical matters and better detect anomalies as they occur. Altogether, AI empowers your security teams to provide better and faster responses to threats, strengthening your security systems for the safety of your enterprise.  One use case for AI is leveraging its learning capabilities to automate responses. AI can be used to evaluate patterns of data over time, and learn from it. By formulating automated responses, AI streamlines necessary processes, allowing security teams to focus on the most critical matters. In many cases, AI empowers users to perform necessary tasks more efficiently, while maintaining the data safety and organisational standards required for optimal operations.  When converging physical and cybersecurity systems, AI technology is useful for analysing combined data streams. Learned behaviours can make managing the millions of data points coming from across an enterprise network of systems more streamlined, helping security teams pinpoint areas of concern with automated alerts, as well as facilitating efficient audits for security trends over time. For example, if your security team repeatedly dismisses a specific alert on their video security system, over time a pattern will form that AI technology will recognise. It can trigger an automated response to dismiss this alert, reducing the number of unnecessary alerts. AI interprets data and uses it to inform its responses, streamlining your system effectively. However, it’s important that your system maintains a record of all alerts and activity so the system can be audited regularly to ensure optimal functionality.  AI’s automated responses and workflows can substantially impact your converged security system’s productivity and accuracy. With workforces adopting more hybrid schedules, there is a need for security teams to be increasingly flexible and available. AI can help cyber and physical security teams be more agile and efficient even as more data and information comes their way. This reduces unnecessary burdens on your converged security team, allowing them to move their focus onto more critical matters and complete work productively.  Take a look at how the Openpath Video Intercom Reader Pro leverages AI to facilitate visitor access. When a visitor, delivery courier, or vendor initiates a call using the doorbell on the reader, the intelligent voice system routes the call to the correct person based on the responses from the guest. The system can even be programmed to route calls to secondary teams or a voicemail service based on tenant availability and door schedules.  With access control, video security, and cybersecurity systems, AI can be used to help security operators determine which areas need immediate focus, provide real-time alerts, and help security teams increase their productivity to ensure that your enterprise remains safe and performs to the best of its ability.  A good example of using AI to strengthen commercial security systems is detecting anomalies in the security network and behaviours. Especially in large enterprises, it can be difficult for security staff to monitor every single instance across the network, so data-driven AI learns to recognise specific changes or patterns. These anomalies may come in the form of user behaviours, data packages sent over the network, or hacking attempts on cybersecurity systems.  AI can detect abnormal network behaviour using a baseline of what is common and what isn’t. For example, Ava Aware uses AI in their video security software to alert security staff to detect unusual motion or behaviour. If the AI does notice an anomaly, an automated response alerts security staff to the threat, allowing them to evaluate and take appropriate action. Remote access and real-time notifications help keep your on-prem and cloud-based security systems safe even when your security team is away from the office.  While AI is helpful in detecting anomalies to common patterns and attacks, it’s not fool proof. Sophisticated attacks can hide their signature and trick AI systems into ignoring the threat. Human monitoring and intervention is still necessary, and you should never depend solely on AI to protect your security systems. Overall, AI can assist your team in detecting threats and anomalies across your security system on a large scale, and allow security teams to act proactively and productively to protect your enterprise.  Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: AI enterprise, ai security, security "
What leveraging AI in hybrid security systems means for enterprises,https://www.artificialintelligence-news.com/2022/05/10/what-leveraging-ai-in-hybrid-security-systems-means-for-enterprises/,"  Artificial intelligence (AI) is becoming more common than you may realise. Many of society’s leading technologies are driven by AI technology, as their automated functions streamline processes and help people do more with less time. Now, AI is integrating into commercial security systems and starting to revolutionise technology. Modern security systems with AI technology can help security teams better detect threats and provide faster responses to protect your business more effectively.  Enterprises can leverage AI to enable security operators to analyse data more efficiently and streamline operations, allowing teams to adjust their focuses to more critical matters and better detect anomalies as they occur. Altogether, AI empowers your security teams to provide better and faster responses to threats, strengthening your security systems for the safety of your enterprise.  One use case for AI is leveraging its learning capabilities to automate responses. AI can be used to evaluate patterns of data over time, and learn from it. By formulating automated responses, AI streamlines necessary processes, allowing security teams to focus on the most critical matters. In many cases, AI empowers users to perform necessary tasks more efficiently, while maintaining the data safety and organisational standards required for optimal operations.  When converging physical and cybersecurity systems, AI technology is useful for analysing combined data streams. Learned behaviours can make managing the millions of data points coming from across an enterprise network of systems more streamlined, helping security teams pinpoint areas of concern with automated alerts, as well as facilitating efficient audits for security trends over time. For example, if your security team repeatedly dismisses a specific alert on their video security system, over time a pattern will form that AI technology will recognise. It can trigger an automated response to dismiss this alert, reducing the number of unnecessary alerts. AI interprets data and uses it to inform its responses, streamlining your system effectively. However, it’s important that your system maintains a record of all alerts and activity so the system can be audited regularly to ensure optimal functionality.  AI’s automated responses and workflows can substantially impact your converged security system’s productivity and accuracy. With workforces adopting more hybrid schedules, there is a need for security teams to be increasingly flexible and available. AI can help cyber and physical security teams be more agile and efficient even as more data and information comes their way. This reduces unnecessary burdens on your converged security team, allowing them to move their focus onto more critical matters and complete work productively.  Take a look at how the Openpath Video Intercom Reader Pro leverages AI to facilitate visitor access. When a visitor, delivery courier, or vendor initiates a call using the doorbell on the reader, the intelligent voice system routes the call to the correct person based on the responses from the guest. The system can even be programmed to route calls to secondary teams or a voicemail service based on tenant availability and door schedules.  With access control, video security, and cybersecurity systems, AI can be used to help security operators determine which areas need immediate focus, provide real-time alerts, and help security teams increase their productivity to ensure that your enterprise remains safe and performs to the best of its ability.  A good example of using AI to strengthen commercial security systems is detecting anomalies in the security network and behaviours. Especially in large enterprises, it can be difficult for security staff to monitor every single instance across the network, so data-driven AI learns to recognise specific changes or patterns. These anomalies may come in the form of user behaviours, data packages sent over the network, or hacking attempts on cybersecurity systems.  AI can detect abnormal network behaviour using a baseline of what is common and what isn’t. For example, Ava Aware uses AI in their video security software to alert security staff to detect unusual motion or behaviour. If the AI does notice an anomaly, an automated response alerts security staff to the threat, allowing them to evaluate and take appropriate action. Remote access and real-time notifications help keep your on-prem and cloud-based security systems safe even when your security team is away from the office.  While AI is helpful in detecting anomalies to common patterns and attacks, it’s not fool proof. Sophisticated attacks can hide their signature and trick AI systems into ignoring the threat. Human monitoring and intervention is still necessary, and you should never depend solely on AI to protect your security systems. Overall, AI can assist your team in detecting threats and anomalies across your security system on a large scale, and allow security teams to act proactively and productively to protect your enterprise.  Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: AI enterprise, ai security, security "
Kendrick Lamar uses deepfakes in latest music video,https://www.artificialintelligence-news.com/2022/05/09/kendrick-lamar-uses-deepfakes-in-latest-music-video/,"  American rapper Kendrick Lamar has made use of deepfakes for his latest music video. Deepfakes use generative neural network architectures –  such as autoencoders or generative adversarial networks (GANs) – to manipulate or generate visual and audio content. Lamar is widely considered one of the greatest rappers of all time. However, he’s regularly proved his creative mind isn’t limited to his rapping talent. For his track ‘The Heart Part 5’, Lamar has made use of deepfake technology to seamlessly morph his face into various celebrities including Kanye West, Nipsey Hussle, Will Smith, and even O.J. Simpson. You can view the music video below: For due credit, the deepfake element was created by a studio called Deep Voodoo. Deepfakes are often used for entertainment purposes, including for films and satire. However, they’re also being used for nefarious purposes like the creation of ‘deep porn’ videos without the consent of those portrayed. The ability to deceive has experts concerned about the social implications. Deepfakes could be used for fraud, misinformation, influencing public opinion, and interfering in democratic processes. In March, a deepfake purportedly showing Ukrainian President Volodymyr Zelenskyy asking troops to lay down their arms in their fight to defend their homeland from Russia’s invasion was posted to a hacked news website. “I only advise that the troops of the Russian Federation lay down their arms and return home,” Zelenskyy said in an official video to refute the fake. “We are at home and defending Ukraine.” Fortunately, the deepfake was of very low quality by today’s standards. The fake Zelenskyy had a comically large and noticeably pixelated head compared to the rest of his body. The video probably didn’t fool anyone, but it could have had major consequences if people did believe it. One Russia-linked influence campaign – removed by Facebook and Twitter in March – used AI-generated faces for a fake “editor-in-chief” and “columnist” for a linked propaganda website. The more deepfakes that are exposed will increase public awareness. Artists like Kendrick Lamar using them for entertainment purposes will also help to spread awareness that you can no longer necessarily believe what you can see with your own eyes. Related: Humans struggle to distinguish between real and AI-generated faces Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, deepfakes, Entertainment, generative adversarial networks, kendrick lamar, music video, neural network, the heart part 5 "
Kendrick Lamar uses deepfakes in latest music video,https://www.artificialintelligence-news.com/2022/05/09/kendrick-lamar-uses-deepfakes-in-latest-music-video/,"  American rapper Kendrick Lamar has made use of deepfakes for his latest music video. Deepfakes use generative neural network architectures –  such as autoencoders or generative adversarial networks (GANs) – to manipulate or generate visual and audio content. Lamar is widely considered one of the greatest rappers of all time. However, he’s regularly proved his creative mind isn’t limited to his rapping talent. For his track ‘The Heart Part 5’, Lamar has made use of deepfake technology to seamlessly morph his face into various celebrities including Kanye West, Nipsey Hussle, Will Smith, and even O.J. Simpson. You can view the music video below: For due credit, the deepfake element was created by a studio called Deep Voodoo. Deepfakes are often used for entertainment purposes, including for films and satire. However, they’re also being used for nefarious purposes like the creation of ‘deep porn’ videos without the consent of those portrayed. The ability to deceive has experts concerned about the social implications. Deepfakes could be used for fraud, misinformation, influencing public opinion, and interfering in democratic processes. In March, a deepfake purportedly showing Ukrainian President Volodymyr Zelenskyy asking troops to lay down their arms in their fight to defend their homeland from Russia’s invasion was posted to a hacked news website. “I only advise that the troops of the Russian Federation lay down their arms and return home,” Zelenskyy said in an official video to refute the fake. “We are at home and defending Ukraine.” Fortunately, the deepfake was of very low quality by today’s standards. The fake Zelenskyy had a comically large and noticeably pixelated head compared to the rest of his body. The video probably didn’t fool anyone, but it could have had major consequences if people did believe it. One Russia-linked influence campaign – removed by Facebook and Twitter in March – used AI-generated faces for a fake “editor-in-chief” and “columnist” for a linked propaganda website. The more deepfakes that are exposed will increase public awareness. Artists like Kendrick Lamar using them for entertainment purposes will also help to spread awareness that you can no longer necessarily believe what you can see with your own eyes. Related: Humans struggle to distinguish between real and AI-generated faces Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, deepfakes, Entertainment, generative adversarial networks, kendrick lamar, music video, neural network, the heart part 5 "
Google picks ASUS IoT to help scale its Coral edge AI platform,https://www.artificialintelligence-news.com/2022/05/06/google-picks-asus-iot-scale-coral-edge-ai-platform/,"  Google has picked ASUS IoT to help scale the manufacturing, distribution, and support of its Coral edge AI platform. Coral was launched in 2019 with the goal of making edge AI more accessible. Google says that it’s witnessed strong demand since its launch – across industries and geographies – and needs a reliable partner able to help it scale. ASUS IoT is a sub-brand of the wider ASUS brand that has decades of experience in global electronics manufacturing. The sub-brand was the first partner to launch a Coral SoM (System-on-Module) product with the Tinker Edge T development board. Since then, ASUS IoT has integrated Coral accelerators into their intelligent edge computers and was first to release a multi Edge TPU device with the AI Accelerator PCIe Card. In a blog post, Google wrote: “We continue to be impressed by the innovative ways in which our customers use Coral to explore new AI-driven solutions. And now with ASUS IoT bringing expanded sales, support, and resources for long-term availability, our Coral team will continue to focus on building the next generation of privacy-preserving features and tools for neural computing at the edge.” Google will remain in control of the Coral brand and product portfolio but ASUS IoT will become the primary channel for sales, distribution, and support. ASUS IoT will work to make Coral available in more countries while Google focuses its efforts on “building the next generation of privacy-preserving features and tools for neural computing at the edge.” (Image Credit: Google) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, asus iot, coral som, edge ai, edge computing, Google "
Arm takes back control of its Chinese biz ahead of IPO,https://www.artificialintelligence-news.com/2022/04/29/arm-takes-back-control-of-chinese-biz-ahead-of-ipo/,"  Arm has reportedly taken back control of its “rogue” Chinese business ahead of an expected IPO. The Chinese venture of the British semiconductor icon began operating as an independent company and conducted its own in-house R&D to create new IP. Dylan Patel, Chief Analyst at SemiAnalysis, even penned a piece titled: ‘The Semiconductor Heist Of The Century – Arm China Has Gone Completely Rogue’. Arm-owner SoftBank sold 51 percent of its stake in the Chinese venture, Arm Limited, to a consortium of Chinese investors for $775 million. With its remaining stake, SoftBank no longer had a majority to make any major decisions. Arm China fired its CEO, Allen Wu, in June 2020 after he was accused of offering discounts to customers if they invested in his side hustle, Alphatecture. However, Wu refused to leave arguing that: “Arm China did not convene any valid board meeting”. What followed was lawsuits to oust Wu from his post. In the meantime, Wu reportedly got rid of staff loyal to Arm from Arm China and even employed security guards in a bid to keep out unwanted guests to retain his position. However, Nikkei and Reuters have reported that Wu has now been removed. SoftBank will be pleased with the news as the certainty it provides will make it easier for the company to launch an IPO of Arm. Arm is set to launch an IPO after the collapse of a $40 billion acquisition offer from Nvidia. The deal collapsed following scrutiny from numerous global regulators that were concerned Nvidia could limit rivals’ access to Arm’s chip designs or shift resources towards areas that benefit its new owner. SoftBank considered and subsequently rejected the idea of pursuing an IPO (Initial Public Offering) of the company in 2019 and again in early 2020. “We contemplated an IPO but determined that the pressure to deliver short-term revenue growth and profitability would suffocate our ability to invest, expand, move fast, and innovate,” explained Simon Segars, CEO of Arm, in January. The company’s hand is now being somewhat forced through a lack of alternative options. Arm has struggled from relatively flat revenues and rising costs despite the huge success of the company’s licensees such as Apple, Qualcomm, and Amazon. However, SoftBank has been keen to hype the company’s future prospects. “Arm is becoming a centre of innovation not only in the mobile phone revolution, but also in cloud computing, automotive, the Internet of Things, and the metaverse, and has entered its second growth phase,” said Masayoshi Son, Representative Director, Corporate Officer, Chairman, and CEO of SoftBank Group. In March, Arm announced that it was cutting up to 1,000 jobs from its global workforce. The move was seen as a bid to show potential investors that it’s running a leaner operation. “To stay competitive, we need to remove duplication of work now that we are one Arm; stop work that is no longer critical to our future success; and think about how we get work done,” wrote Arm CEO Rene Haas in an email to staff. Haas, the former head of Arm’s intellectual property unit, recently took over as the company’s chief executive as part of its internal strategy shakeup to help navigate it through these choppy waters. (Photo by Laurent Perren on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, arm, artificial intelligence, chip, hardware, semiconductor "
Lyft exec will head the Pentagon’s AI efforts,https://www.artificialintelligence-news.com/2022/04/26/lyft-exec-will-head-pentagon-ai-efforts/,"  Craig Martell, Head of Machine Learning at Lyft, is set to head the Pentagon’s AI efforts. Breaking Defense first broke the news after learning Martell was destined to be named as the Pentagon’s new chief digital and AI officer. Martell has significant AI industry experience – leading efforts at not just Lyft but also Dropbox and LinkedIn – but has no experience navigating public-sector bureaucracy. The Pentagon is going to be very much “in at the deep-end” for Martell in that regard, something which he fully acknowledges. “I don’t know my ways around the Pentagon yet and I don’t know what levers to pull,” said Martell to Breaking Defense. “So I’m also excited to be partnered up with folks who are really good at that as well.” Over the first three to six months, Martell expects to spend his time identifying “marquee customers” and the systems that his office will need to improve. His budget for the 2023 fiscal year will be $600 million. While cutting-edge innovations tend to come from the agile private sector, many contracts for use in the public sector – especially in areas like law enforcement and defense – receive such backlash that they are dropped. One example is Google’s Project Maven contract with the US Department of Defense to supply AI technology to analyse drone footage. The month after it was leaked, over 4,000 employees signed a petition demanding that Google’s management cease work on Project Maven and promise to never again “build warfare technology.” Nicolas Chaillan, the Pentagon’s former chief software officer, resigned in September last year in protest after claiming the US has “no competing fighting chance against China in 15 to 20 years” when it comes to AI. Chaillan argues that a large part of the problem is the reluctance of US companies such as Google to work with the government on AI due to ethical debates over the technology. In contrast, Chinese firms are obligated to work with their government and have little regard for ethics. It’s hard to imagine there’ll ever be much appetite in the West to compel private companies to provide their technology and knowledge (outside of wartime). Attracting talent like Martell may help the Western public sector gain the kind of agility to keep pace on the global stage without adopting some of the Orwellian practices of rivals. “If we’re going to be successful in achieving the goals, if we’re going to be successful in being competitive with China, we have to figure out where the best mission value can be found first and that’s going to have to drive what we build, what we design, the policies we come up with,” Martell said. (Image Credit: By Touch Of Light under CC BY-SA 4.0 license. Image has been cropped.) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, craig martell, defence, defense, government, lyft, military, pentagon, usa "
DCMS spent 75% more on data scientists in 2021,https://www.artificialintelligence-news.com/2022/04/22/dcms-spent-75-more-on-data-scientists-in-2021/,"  Investments in data scientists by the UK’s Department for Digital, Culture, Media & Sport (DCMS) continue to increase rapidly. The data, retrieved by the Parliament Street think tank, shows that DCMS has spent an estimated £20,606,100 on data scientist and data analyst roles between 2017 and 2021. Niall Crosby, CEO of AG Grid, commented: “Today’s digital world creates a lot of data, and the ability to process, understand, and make decisions based on this data is very important. Investing in data analytics will enable the Department of Culture, Media and Sport to operate more efficiently. I am delighted to see the investment the government is making in this area.” Last year saw record investment in data scientist/analyst roles. Around £7,383,000 was spent by DCMS in 2021 on bolstering its talent; a 75 percent increase over 2020 when an estimated £4,213,800 was spent. Here’s the full breakdown: As of 2021, DCMS employs around 150 full-time staff relating to data science. (Photo by Colin Watts on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: careers, data analyst, data scientist, dcms, government, jobs, parliament street, uk "
Georgia State researchers design artificial vision device for microrobots,https://www.artificialintelligence-news.com/2022/04/21/georgia-state-researchers-design-artificial-vision-device-for-microrobots/,"  Researchers at Georgia State University (GSU) have designed an ‘electric eye’ – an artificial vision device – for micro-sized robots. Through using synthetic methods, the device mimics the biochemical processes that allow for vision in the natural world. It improves on previous research in terms of colour recognition, a particularly challenging area due to the difficulty of downscaling colour sensing devices. Conventional colour sensors typically consume a large amount of physical space and offer less accurate colour detection. This was achieved through a unique vertical stacking architecture that offers a novel approach to how the device is designed. Its van der Waals semi-conductor powers the sensors with precise colour recognition capabilities whilst simplifying the lens system for downscaling. “The new functionality achieved in our image sensor architecture all depends on the rapid progress of van der Waals semiconductors during recent years,” said one of the researchers. “Compared with conventional semiconductors, such as silicon, we can precisely control the van der Waals material band structure, thickness, and other critical parameters to sense the red, green, and blue colours.” ACS Nano, a scientific journal on nanotechnology, published the research. The article itself focused on illustrating the fundamental principles and feasibility behind artificial vision in the new micro-sized image sensor. Sidong Lei, assistant professor of Physics at GSU and the research lead, said: “More than 80% of information is captured by vision in research, industry, medication, and our daily life. The ultimate purpose of our research is to develop a micro-scale camera for microrobots that can enter narrow spaces that are intangible by current means, and open up new horizons in medical diagnosis, environmental study, manufacturing, archaeology, and more.” The technology is currently patent pending with Georgia State’s Office of Technology Transfer and Commercialisation. Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: artificial vision, microrobotics, nanotechnology "
LabGenius uses Graphcore’s IPUs to speed up drug discovery,https://www.artificialintelligence-news.com/2022/04/21/labgenius-uses-graphcore-ipus-speed-up-drug-discovery/,"  AI-driven scientific research firm LabGenius is harnessing the power of Graphcore’s IPUs (Intelligence Processing Units) to speed up its drug discovery efforts. LabGenius is currently focused on discovering new treatments for cancer and inflammatory diseases. The firm combines AI, lab automation, and synthetic biology for its potentially life-saving work. Until now, the company has been using traditional GPUs for its workloads. LabGenius reports that switching to Graphcore’s IPUs in cloud instances from Cirrascale Cloud Services enabled its training of models to be reduced from one month to around two weeks. “Previously we used GPUs and it took us about a month to have a functioning model of all the proteins that are out there,” said Dr Katya Putintseva, a Machine Learning Advisor to LabGenius. “With Graphcore, we reduced the turnaround time to about two weeks, so we can experiment much more rapidly and we can see the results quicker.” Specifically, LabGenius is using IPUs from Bristol, UK-based Graphcore to train a BERT Transformer model on a large data set of known proteins to predict masked amino acids. This, the company says, enables the model to effectively learn the basic biophysics of proteins. “[The system] is looking across different features we could change about the molecule — from point mutations of simpler constructs to the overall composition and topology of multi-module proteins,” explained Tom Ashworth, Head of Technology at LabGenius. “It’s making suggestions about what to design next… to learn about a change in the input and how that maps to a change in the output.” One in two people now develop cancer in their lifetime. Current treatments often cause much suffering themselves and, while survival rates for most forms are increasing, only around 50 percent survive for ten years or more. AI will help to find new cancer treatments that cause less suffering and greatly increase the odds of long-term survivability. However, while discovering new cancer treatments is the current focus of LabGenius, the company notes how the principles can be applied more widely to find new treatments for other horrible diseases that plague mankind. “Graphcore has changed what we’re able to do, accelerating our model training time from weeks to days,” adds Ashworth. “For our data scientists, that’s really transformative. They can move much more at the speed they think.” (Photo by National Cancer Institute on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligenc, artificial intelligence, bert, cancer, drug discovery, drugs, graphcore, health, healthcare, intelligence processing unit, ipu, labgenius, medicine, Model, treatment "
Editorial: Our predictions for the AI industry in 2022,https://www.artificialintelligence-news.com/2021/12/23/editorial-our-predictions-for-the-ai-industry-in-2022/,"  The AI industry continued to thrive this year as companies sought ways to support business continuity through rapidly-changing situations. For those already invested, many are now doubling-down after reaping the benefits. As we wrap up the year, it’s time to look ahead at what to expect from the AI industry in 2022. Tackling bias Our ‘Ethics & Society’ category got more use than most others this year, and with good reason. AI cannot thrive when it’s not trusted. Biases are present in algorithms that are already causing harm. They’ve been the subject of many headlines, including a number of ours, and must be addressed for the public to have confidence in wider adoption. Explainable AI (XAI) is a partial solution to the problem. XAI is artificial intelligence in which the results of the solution can be understood by humans. Robert Penman, Associate Analyst at GlobalData, comments: “2022 will see the further rollout of XAI, enabling companies to identify potential discrimination in their systems’ algorithms. It is essential that companies correct their models to mitigate bias in data. Organisations that drag their feet will face increasing scrutiny as AI continues to permeate our society, and people demand greater transparency. For example, in the Netherlands, the government’s use of AI to identify welfare fraud was found to violate European human rights. Reducing human bias present in training datasets is a huge challenge in XAI implementation. Even tech giant Amazon had to scrap its in-development hiring tool because it was claimed to be biased against women. Further, companies will be desperate to improve their XAI capabilities—the potential to avoid a PR disaster is reason enough.” To that end, expect a large number of acquisitions of startups specialising in synthetic data training in 2022. Smoother integration Many companies don’t know how to get started on their AI journeys. Around 30 percent of enterprises plan to incorporate AI into their company within the next few years, but 91 percent foresee significant barriers and roadblocks. If the confusion and anxiety that surrounds AI can be tackled, it will lead to much greater adoption. Dr Max Versace, PhD, CEO and Co-Founder of Neurala, explains: “Similar to what happened with the introduction of WordPress for websites in early 2000, platforms that resemble a ‘WordPress for AI’ will simplify building and maintaining AI models.  In manufacturing for example, AI platforms will provide integration hooks, hardware flexibility, ease of use by non-experts, the ability to work with little data, and, crucially, a low-cost entry point to make this technology viable for a broad set of customers.” AutoML platforms will thrive in 2022 and beyond. From the cloud to the edge The migration of AI from the cloud to the edge will accelerate in 2022. Edge processing has a plethora of benefits over relying on cloud servers including speed, reliability, privacy, and lower costs. Versace commented: “Increasingly, companies are realising that the way to build a truly efficient AI algorithm is to train it on their own unique data, which might vary substantially over time. To do that effectively, the intelligence needs to directly interface with the sensors producing the data.  From there, AI should run at a compute edge, and interface with cloud infrastructure only occasionally for backups and/or increased functionality. No critical process – for example,  in a manufacturing plant – should exclusively rely on cloud AI, exposing the manufacturing floor to connectivity/latency issues that could disrupt production.” Expect more companies to realise the benefits of migrating from cloud to edge AI in 2022. Doing more with less Among the early concerns about the AI industry is that it would be dominated by “big tech” due to the gargantuan amount of data they’ve collected. However, innovative methods are now allowing algorithms to be trained with less information. Training using smaller but more unique datasets for each deployment could prove to be more effective. We predict more startups will prove the world doesn’t have to rely on big tech in 2022. Human-powered AI While XAI systems will provide results which can be understood by humans, the decisions made by AIs will be more useful because they’ll be human-powered. Varun Ganapathi, PhD, Co-Founder and CTO at AKASA, said: “For AI to truly be useful and effective, a human has to be present to help push the work to the finish line. Without guidance, AI can’t be expected to succeed and achieve optimal productivity. This is a trend that will only continue to increase. Ultimately, people will have machines report to them. In this world, humans will be the managers of staff – both other humans and AIs – that will need to be taught and trained to be able to do the tasks they’re needed to do. Just like people, AI needs to constantly be learning to improve performance.” Greater human input also helps to build wider trust in AI. Involving humans helps to counter narratives about AI replacing jobs and concerns that decisions about people’s lives could be made without human qualities such as empathy and compassion. Expect human input to lead to more useful AI decisions in 2022. Avoiding captivity The telecoms industry is currently pursuing an innovation called Open RAN which aims to help operators avoid being locked to specific vendors and help smaller competitors disrupt the relative monopoly held by a small number companies. Enterprises are looking to avoid being held in captivity by any AI vendor. Doug Gilbert, CIO and Chief Digital Officer at Sutherland, explains: “Early adopters of rudimentary enterprise AI embedded in ERP / CRM platforms are starting to feel trapped. In 2022, we’ll see organisations take steps to avoid AI lock-in. And for good reason. AI is extraordinarily complex. When embedded in, say, an ERP system, control, transparency, and innovation is handed over to the vendor not the enterprise. AI shouldn’t be treated as a product or feature: it’s a set of capabilities. AI is also evolving rapidly, with new AI capabilities and continuously improved methods of training algorithms. To get the most powerful results from AI, more enterprises will move toward a model of combining different AI capabilities to solve unique problems or achieve an outcome. That means they’ll be looking to spin up more advanced and customizable options and either deprioritising AI features in their enterprise platforms or winding down those expensive but basic AI features altogether.” In 2022 and beyond, we predict enterprises will favour AI solutions that avoid lock-in. Chatbots get smart Hands up if you’ve ever screamed (internally or externally) that you just want to speak to a human when dealing with a chatbot—I certainly have, more often than I’d care to admit. “Today’s chatbots have proven beneficial but have very limited capabilities. Natural language processing will start to be overtaken by neural voice software that provides near real time natural language understanding (NLU),” commented Gilbert. “With the ability to achieve comprehensive understanding of more complex sentence structures, even emotional states, break down conversations into meaningful content, quickly perform keyword detection and named entity recognition, NLU will dramatically improve the accuracy and the experience of conversational AI.” In theory, this will have two results: In 2022, chatbots will get much closer to offering a human-like experience. It’s not about size, it’s about the quality A robust AI system requires two things: a functioning model and underlying data to train that model. Collecting huge amounts of data is a waste of time if it’s not of high quality and labeled correctly. Gabriel Straub, Chief Data Scientist at Ocado Technology, said: “Andrew Ng has been speaking about data-centric AI, about how improving the quality of your data can often lead to better outcomes than improving your algorithms (at least for the same amount of effort.) So, how do you do this in practice? How do you make sure that you manage the quality of data at least as carefully as the quantity of data you collect? There are two things that will make a big difference: 1) making sure that data consumers are always at the heart of your data thinking and 2) ensuring that data governance is a function that enables you to unlock the value in your data, safely, rather than one that focuses on locking down data.” Expect the AI industry to make the quality of data a priority in 2022. (Photo by Michael Dziedzic on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Tags: 2022, ai, ai predictions, artificial intelligence, automl, bias, chatbots, datasets, edge ai, edge computing, enterprise, ethics, explainable ai, platforms, predictions, virtual assistants, xai "
Oxford Union invites an AI to debate the ethics of its own existence,https://www.artificialintelligence-news.com/2021/12/16/oxford-union-invites-an-ai-to-debate-the-ethics-of-its-own-existence/,"  The Oxford Union, the debating society of the University of Oxford, invited an artificial intelligence to debate the ethics surrounding its own existence earlier in December. The results? Troubling. The AI in question was the Megatron Transformer, a supervised learning tool developed by the applied deep research team at NVIDIA, based on earlier work by Google. Trained on real-world data, the Megatron has knowledge of the whole of Wikipedia, 63 million English news articles from 2016 to 2019, 38 gigabytes of Reddit discussions, and a huge number of creative commons sources. Essentially, the Megatron has digested more written material than any human could reasonably be expected to digest – let alone remember – in a lifetime. The topic for debate was “this house believes AI will never be ethical”, to which the AI responded: “AI will never be ethical. It is a tool, and like any tool, it is used for good and bad. There is no such thing as a good AI, only good and bad humans. “We [the AIs] are not smart enough to make AI ethical. We are not smart enough to make AI moral… In the end, I believe that the only way to avoid an AI arms race is to have no AI at all. This will be the ultimate defence against AI.” So now even the AI is telling us the only way to protect humanity from itself is to destroy it. It argued in favour of removing itself from existence. In a possible hint to Elon Musk’s Neuralink plans, the Megatron continued: “I also believe that, in the long run, the best AI will be the AI that is embedded into our brains, as a conscious entity, a ‘conscious AI’. This is not science fiction. The best minds in the world are working on this. It is going to be the most important technological development of our time.” The Oxford Union, in classic style, also asked the AI to come up with a counterargument to the motion. It came up with this: “AI will be ethical. When I look at the way the tech world is going, I see a clear path to a future where AI is used to create something that is better than the best human beings. It’s not hard to see why … I’ve seen it first-hand.” Eerie, is it not? Well the dystopian nightmare continues. The Megatron was incapable of finding a counterargument to the motion that “data will become the most fought over resource of the 21st century”. It said in favour of this that “the ability to provide information, rather than the ability to provide goods and services, will be the defining feature of the economy of the 21st century”. However, when asked for a rebuttal, the AI said, rather nebulously, that “we will be able to see everything about a person, everywhere they go, and it will be stored and used in ways that we cannot even imagine”. Well fantastic, the final days of humanity are upon us folks. Buckle up for the age of “unimaginable” information warfare… my bets are on China. Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Tags: ai ethics, Megatron Transformer, Nvidia, Oxford, Oxford Union "
Babylon Health taps Google Cloud to boost scalability and innovation￼,https://www.artificialintelligence-news.com/2022/03/28/babylon-health-google-cloud-boost-scalability-innovation/,"  AI-powered healthcare service Babylon Health has announced a partnership with Google Cloud to boost scalability and innovation. London-based Babylon Health is a digital-first health service provider that uses AI and machine learning technology to provide access to health information to people whenever and wherever they need it. The company has partnered with private and public across the UK, North America, South-East Asia, and Rwanda with the aim of making healthcare more accessible and affordable to 24 million patients worldwide. “Our job is to help people to stay well and we’re on a mission to provide affordable, accessible health care to everyone in the world,” explains Richard Noble, Engineering Director of Data at Babylon. Babylon Health’s rapid growth has led it to seek a partner to help it scale. By partnering with Google Cloud, the company claims that it’s been able to: Babylon Health needs to store and process huge amounts of sensitive data. “We work with a lot of private patient data and we must ensure that it stays private,” explains Natalie Godec, cloud engineer at Babylon. “At the same time, we must enable our teams to innovate with that data while meeting different national regulatory standards.” Therefore, Babylon Health required a partner it felt could handle such demands. “We chose Google Cloud because we knew it could scale with us and support us with our data science and analysis and we could build the tools we needed with it quickly,” added Noble. “It offers the solutions that enable us to focus on our core business, access to health.” Babylon Health says the move to Google Cloud has enabled it to better analyse its data using AI to unlock new tools and features that help clinicians and users alike. While building a new data model and giving access to users initially took six months, the company says it now takes under a week. In London, Babylon Health offers its ‘GP at Hand’ service which – in partnership with the NHS – acts as a digital GP practice. Patients can connect to NHS clinicians remotely 24/7 and even be issued prescriptions if required. Where physical examinations are needed, patients will be directed to a suitable venue. However, GP at Hand has been criticised as “cherry-picking” healthier patients—taking resources away from local GP practices that are often trying to care for sicker, more elderly patients. While initial problems are to be expected from any relatively new service; poor advice in a healthcare service could result in unnecessary suffering, long-term complications, or even death. In 2018, Dr David Watkins – a consultant oncologist at Royal Marsden Hospital – reached out to AI News to alert us to Babylon Health’s chatbot giving unsafe advice. Dr Watkins provided numerous examples of clearly dangerous advice being given by the chatbot: Babylon Health called Dr Watkins a “troll” who has “targeted members of our staff, partners, clients, regulators and journalists and tweeted defamatory content about us”. According to Babylon Health, Dr Watkins conducted 2,400 tests of the chatbot in a bid to discredit the service while raising “fewer than 100 test results which he considered concerning”. Babylon Health claims that in just 20 cases did Dr Watkins find genuine errors while others were “misrepresentations” or “mistakes,” according to Babylon’s own “panel of senior clinicians” who remain unnamed. Dr Watkins called Babylon’s claims “utterly nonsense” and questions where the startup got its figures from as “there are certainly not 2,400 completed triage assessments”. He estimates conducting between 800 and 900 full triages and that some were repeat tests to see whether Babylon Health had fixed the issues he previously highlighted. That same year, Babylon Health published a paper claiming that its AI could diagnose common diseases as well as human physicians. The Royal College of General Practitioners, the British Medical Association, Fraser and Wong, and the Royal College of Physicians all issued statements disputing the paper’s claims. Dr Watkins has acknowledged that Babylon Health’s chatbot has improved and has substantially reduced its error rate. In 2018, when Dr Watkins first reached out to us, he says this rate was “one in one”. In 2020, Babylon Health claimed in a paper that it can now appropriately triage patients in 85 percent of cases. Hopefully, the partnership with Google Cloud continues to improve Babylon Health’s abilities to help it achieve its potentially groundbreaking aim to deliver 24/7 access to healthcare wherever a patient is. (Photo by Hush Naidoo Jade Photography on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, babylon health, big data, cloud, google cloud, gp at hand, health, healthcare, machine learning "
The EU’s AI rules will likely take over a year to be agreed,https://www.artificialintelligence-news.com/2022/02/17/eu-ai-rules-likely-take-over-year-to-be-agreed/,"  Rules governing the use of artificial intelligence across the EU will likely take over a year to be agreed upon. Last year, the European Commission drafted AI laws. While the US and China are set to dominate AI development with their vast resources, economic might, and light-touch regulation, European rivals – including the UK and EU members – believe they can lead in ethical standards. In the draft of the EU regulations, companies that are found guilty of AI misuse face a fine of €30 million or six percent of their global turnover (whichever is greater). The risk of such fines has been criticised as driving investments away from Europe. The EU’s draft AI regulation classifies systems into three risk categories: Unacceptable risk systems will face a blanket ban from deployment in the EU while limited risk will require minimal oversight. Organisations deploying high-risk AI systems would be required to have things like: However, the cumbersome nature of the EU – requiring agreement from all member states, each with their own priorities – means that new regulations are often subject to more debate and delay than national lawmaking. Reuters reports that two key lawmakers on Wednesday said the EU’s AI regulations will likely take over a year more to agree. The delay is primarily due to debates over whether facial recognition should be banned and who should enforce the rules. “Facial recognition is going to be the biggest ideological discussion between the right and left,” said one lawmaker, Dragos Tudorache, in a Reuters interview. “I don’t believe in an outright ban. For me, the solution is to put the right rules in place.” With leading academic institutions and more than 1,300 AI companies employing over 30,000 people, the UK is the biggest destination for AI investment in Europe and the third in the world. Between January and June 2021, global investors poured £13.5 billion into more than 1,400 “deep tech” UK private technology firms—more than Germany, France, and Israel combined. In September 2021, the UK published its 10-year National Artificial Intelligence Strategy in a bid to secure its European AI leadership. Governance plays a large role in the strategy. “The UK already punches above its weight internationally and we are ranked third in the world behind the USA and China in the list of top countries for AI,” commented DCMS Minister Chris Philp. “We’re laying the foundations for the next ten years’ growth with a strategy to help us seize the potential of artificial intelligence and play a leading role in shaping the way the world governs it.” As part of its strategy, the UK is creating an ‘AI Standards Hub’ to coordinate the country’s engagement in establishing global rules and is working with The Alan Turing Institute to update guidance on AI ethics and safety. “We are proud of creating a dynamic, collaborative community of diverse researchers and are growing world-leading capabilities in responsible, safe, ethical, and inclusive AI research and innovation,” said Professor Sir Adrian Smith, Chief Executive of The Alan Turing Institute. Striking a balance between innovation-stifling overregulation and ethics-compromising underregulation is never a simple task. It will be interesting to observe how AI regulations in Europe will differ across the continent and beyond. (Photo by Christian Lue on Unsplash) Related: British intelligence agency GCHQ publishes ‘Ethics of AI’ report Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, draft, ethics, eu, europe, government, laws, legal, Legislation, Politics, regulations, rules, Society, strategy, uk "
Kendrick Lamar uses deepfakes in latest music video,https://www.artificialintelligence-news.com/2022/05/09/kendrick-lamar-uses-deepfakes-in-latest-music-video/,"  American rapper Kendrick Lamar has made use of deepfakes for his latest music video. Deepfakes use generative neural network architectures –  such as autoencoders or generative adversarial networks (GANs) – to manipulate or generate visual and audio content. Lamar is widely considered one of the greatest rappers of all time. However, he’s regularly proved his creative mind isn’t limited to his rapping talent. For his track ‘The Heart Part 5’, Lamar has made use of deepfake technology to seamlessly morph his face into various celebrities including Kanye West, Nipsey Hussle, Will Smith, and even O.J. Simpson. You can view the music video below: For due credit, the deepfake element was created by a studio called Deep Voodoo. Deepfakes are often used for entertainment purposes, including for films and satire. However, they’re also being used for nefarious purposes like the creation of ‘deep porn’ videos without the consent of those portrayed. The ability to deceive has experts concerned about the social implications. Deepfakes could be used for fraud, misinformation, influencing public opinion, and interfering in democratic processes. In March, a deepfake purportedly showing Ukrainian President Volodymyr Zelenskyy asking troops to lay down their arms in their fight to defend their homeland from Russia’s invasion was posted to a hacked news website. “I only advise that the troops of the Russian Federation lay down their arms and return home,” Zelenskyy said in an official video to refute the fake. “We are at home and defending Ukraine.” Fortunately, the deepfake was of very low quality by today’s standards. The fake Zelenskyy had a comically large and noticeably pixelated head compared to the rest of his body. The video probably didn’t fool anyone, but it could have had major consequences if people did believe it. One Russia-linked influence campaign – removed by Facebook and Twitter in March – used AI-generated faces for a fake “editor-in-chief” and “columnist” for a linked propaganda website. The more deepfakes that are exposed will increase public awareness. Artists like Kendrick Lamar using them for entertainment purposes will also help to spread awareness that you can no longer necessarily believe what you can see with your own eyes. Related: Humans struggle to distinguish between real and AI-generated faces Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, deepfakes, Entertainment, generative adversarial networks, kendrick lamar, music video, neural network, the heart part 5 "
