Title,Link,Body,Keywords
AI start-up CEO encourages fellow founders to make culture their secret ingredient,https://www.artificialintelligence-news.com/2022/05/30/ai-start-up-ceo-fellow-founders-make-culture-secret-ingredient/,"  The co-founder of a fast-growing conversational AI start-up attributes its ongoing success to the commitment to building a diverse and trusting company culture almost as much as to the quality of the technology it is offering. Andrei Papancea is CEO of conversational AI specialist NLX, which has expanded from five to 25 staff in a little over a year. The small team is geographically spread across the world, from New York to Seattle to Queensland, and Berlin, and speaks 19 different languages, including Arabic, Mandarin, Korean and Spanish. Andrei says that his mission is to combine the best of AI with the best of human support to create extraordinary, memorable self-service experiences for users by building the world’s go-to platform to create human conversational AI applications. “In all the jobs I had throughout my career, I always disliked it when good people – my colleagues, my peers, and my friends – quit. They always left for one of three reasons: they weren’t paid well, they didn’t feel heard or respected, or they didn’t have interesting and engaging work to do. In building NLX, I’ve done the best I can to avoid losing good people because of these three reasons,” explains Andrei. “A lot of our advisors and investors told me about the importance of culture, but nobody really articulated, or there was no cohesive approach to how you get there. And while what we did has resulted in a culture that is purposeful, we didn’t necessarily do it in the pursuit of a good culture or at least not knowingly. We did it because we’ve seen what didn’t work at other companies and didn’t want to make the same mistakes that those other companies made.” Andrei says that you’ll hear a lot of business leaders say that culture is very important and the essence of it is simple – take care of your people. Recently, employee intranet platform Jostle recently set out to find whether it was true that companies in the tech industry have the best company culture. Examining Glassdoor Company Reviews and MIT’s Culture 500. Their analysis found that tech companies have a better company culture according to the Culture 500 sentiment analysis data and, based on nine cultural values identified by Glassdoor and MIT – including agility, diversity, and integrity – they found that tech companies received higher scores across most categories; that they excelled in the Innovation, Agility, and Executioncategories, but fell behind companies in other sectors when it comes to Integrity, Diversity, and Customer. And whilst founders always have the best interests of their company at heart, Andrei says it can be hard to get everyone else on the exact same page. “I guess this is one of the other positive ramifications of a good culture because then everyone else becomes a warrior on behalf of the company in the same direction. And it’s just super powerful to see because then I can focus more on other things, less so on the nitty-gritty day-to-day details, because I know I have an army of people who know exactly what we are fighting for and what we’re trying to achieve. I’m just plugging into that.” (Editor’s note: This article is in association with NLX) ","['employee intranet', 'attributes ongoing', '” andrei', 'analysis found', 'guess positive', 'andrei says', 'integrity –', 'based nine', '’ seen', 'examining glassdoor']"
Darktrace CEO calls for a ‘Tech NATO’ amid growing cyber threats,https://www.artificialintelligence-news.com/2022/05/27/darktrace-ceo-calls-for-a-tech-nato-amid-growing-cyber-threats/,"  The CEO of AI cybersecurity firm Darktrace has called for a “Tech NATO” to counter growing cybersecurity threats. Poppy Gustafsson spoke on Wednesday at the Royal United Services Institute (RUSI) – the UK’s leading and world’s oldest defense think thank – on the evolving cyber threat landscape. Russia’s illegal and unprovoked invasion of Ukraine has led to a global rethinking of security.  While some in the West had begun questioning the need for NATO post-cold war, and many members have failed to meet their defense spending commitments, the invasion of Ukraine has proven why the defense alliance remains a bedrock of Western security. NATO members are now spending more on defense, increasing cooperation, and the alliance is now preparing to accept Sweden and Finland into its fold. Russia has thrown out the rule book with its conduct and will eventually face war crime trials as a result. NATO members, in contrast, have acted in accordance with the UN charter and only provided resources to Ukraine that it can use to defend its territory from the invaders. However, any provision of long-range weapons that could pose a threat to Moscow would be seen as going beyond helping an ally to defend itself into helping attack Russia itself—likely triggering a disastrous global conflict. Those kinds of norms around conventional warfare are well-established. In the cybersphere, they’re yet to be set. “There remains a persistent lack of clarity around how we define an act of war in the cybersphere,” said Gustafsson. Gustafsson wants to see the creation of a dedicated international cyber task force, or a “tech NATO”, where global partners can collaborate, agree, and ratify norms for the cybersphere—including what kind of response would be warranted for breaches. At the beginning of Russia’s invasion, the country attacked Viasat to disable Ukrainian communications. The attack spilt over into other European countries, including rendering 5,800 Enercon wind turbines in Germany unable to communicate for remote monitoring or control. “The attack on the Viasat satellite that disabled Ukrainian military communications one hour before the invasion was a key component of the beginning of this war,” added Gustafsson. “We have seen UK, US, and EU officials jointly attribute this attack to Russia, an immensely political act. That is unprecedented.” No-one reasonable would suggest that incident is worth triggering a full-scale war between NATO and Russia, but clarity is needed on what the response should be. If a cyberattack leads to serious loss of life, should it have any different response than if it was a missile? “There is a shocking tolerance for cyberattacks, and that has to change,” argued Gustafsson. “Organisations that are custodians of valuable, private data can not be allowed to let that data fall into criminal hands through negligence and face no consequences.” Darktrace says it has witnessed a global increase in attacks on critical national infrastructure bodies across its customer base—including a 90 percent increase in high priority security incidents on the networks of energy companies in Europe during the initial week of Russia’s invasion. “Issues that we had thought about speculatively have now become our reality. We are facing war in Europe and there is an essential cyber component both to the way it is being fought and to its international ramifications,” says Professor Madeline Carr, Senior RUSI Associate Fellow and Professor of Global Politics and Cybersecurity at University College London. “This is a complex area which is the subject of a wealth of academic debate and what is needed is clarity, consensus, and cooperation.” Greater cooperation is certainly needed to combat evolving cyber threats. However, Gustafsson’s call for a “Tech NATO” is surprising—not least because NATO itself already has one in the form of the CCDCOE (Cooperative Cyber Defence Centre of Excellence). Despite being run by NATO, the CCDCOE is open to “like-minded non-NATO nations”. Earlier this month, non-NATO member South Korea joined the organisation alongside NATO members Canada and Luxembourg. In March, Ukraine also joined the CCDCOE despite not being a full NATO member. “Cooperation, sharing of information, skills, and best practices are essential for tackling the challenges we face in cyberspace,” said a spokesperson for the Embassy of the Grand Duchy of Luxembourg, following the country’s admission to the CCDCOE. The CCDCOE leans more towards collaboration between public agencies but also brings together representatives from academia and the private sector to discuss cyber norms and improve members’ defenses. “Each member of the CCDCOE plays an important role in building and advancing a strong and efficient unity against cyber threats,” explained Colonel Jaak Tarien, Head of the CCDCOE. “In the long run, the conditions for peace in the cyber realm and a response to the security threats to the modern world cannot be created without united and committed support.” We’ve reached out to Darktrace for clarification on Gustafsson’s call for a “Tech NATO” and how it would differ from the CCDCOE. We presume it would have a greater focus on private sector companies like Darktrace but will update this article when/if we receive an official response. Related: US disrupts large Russian botnet ‘before it could be used’ Want to learn more about cybersecurity and the cloud from industry leaders? Check out Cyber Security & Cloud Expo taking place in Amsterdam, California, and London. The event is being co-hosted with the AI & Big Data Expo. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ccdcoe, cyber security, cyberattack, cybersecurity, darktrace, nato, Poppy Gustafsson, Royal United Services Institute, rusi, russia, security, viasat ","['” darktrace', 'ccdcoe leans', 'range weapons', 'us disrupts', '– uk', 'conditions peace', 'ceo ai', '’ reached', 'presume would', 'member ccdcoe']"
Deepfakes are now being used to help solve crimes,https://www.artificialintelligence-news.com/2022/05/25/deepfakes-are-now-being-used-to-help-solve-crimes/,"  A deepfake video created by Dutch police could help to change the often negative perception of the technology. Deepfakes use generative neural network architectures – such as autoencoders or generative adversarial networks (GANs) – to manipulate or generate visual and audio content. The technology is already being used for malicious purposes including generating sexual content of individuals without their consent, fraud, and the creation of deceptive content aimed at changing views and influencing democratic processes. However, authorities in Rotterdam have proven the technology can be put to use for good. Dutch police have created a deepfake video of 13-year-old Sedar Soares – a young footballer who was shot dead in 2003 while throwing snowballs with his friends in the car park of a Rotterdam metro station – in an appeal for information to finally solve his murder. The video depicts Soares picking up a football in front of the camera and walking through a guard of honour on the field that comprises his relatives, friends, and former teachers. “Somebody must know who murdered my darling brother. That’s why he has been brought back to life for this film,” says a voice in the video, before Soares drops his ball. “Do you know more? Then speak,” his relatives and friends say, before his image disappears from the field. The video then gives the police contact details. It’s hoped the stirring video and a reminder of what Soares would have looked like at the time will help to jog memories and lead to the case finally being solved. Daan Annegarn, a detective with the National Investigation Communications Team, said: “We know better and better how cold cases can be solved. Science shows that it works to hit witnesses and the perpetrator in the heart—with a personal call to share information. What better way to do that than to let Sedar and his family do the talking?  We had to cross a threshold. It is not nothing to ask relatives: ‘Can I bring your loved one to life in a deepfake video? We are convinced that it contributes to the detection, but have not done it before.‘ The family has to fully support it.” So far, it seems to have had an impact. The police claim to have already received dozens of tips but they need to see whether they’re credible. In the meantime, anyone that may have any information is encouraged to come forward. “The deployment of deepfake is not just a lucky shot. We are convinced that it can touch hearts in the criminal environment—that witnesses and perhaps the perpetrator can come forward,” Annegarn concludes. Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: deepfake, generative adversarial networks, netherlands, policing, rotterdam, sedar soares ","['old sedar', '’ hoped', 'video depicts', 'technology already', 'deepfake video', 'police claim', 'deepfakes use', 'creation deceptive', 'explore upcoming', 'science shows']"
UK fines Clearview AI £7.5M for scraping citizens’ data,https://www.artificialintelligence-news.com/2022/05/23/uk-fines-clearview-ai-7-5m-for-scraping-citizens-data/,"  Clearview AI has been fined £7.5 million by the UK’s privacy watchdog for scraping the online data of citizens without their explicit consent. The controversial facial recognition provider has scraped billions of images of people across the web for its system. Understandably, it caught the attention of regulators and rights groups from around the world. In November 2021, the UK’s Information Commissioner’s Office (ICO) imposed a potential fine of just over £17 million on Clearview AI. Today’s announcement suggests Clearview AI got off relatively lightly. John Edwards, UK Information Commissioner, said: “Clearview AI Inc has collected multiple images of people all over the world, including in the UK, from a variety of websites and social media platforms, creating a database with more than 20 billion images. The company not only enables identification of those people, but effectively monitors their behaviour and offers it as a commercial service. That is unacceptable. That is why we have acted to protect people in the UK by both fining the company and issuing an enforcement notice.” The enforcement notice requires Clearview AI to delete all facial recognition data. A joint investigation by the UK’s ICO and the Office of the Australian Information Commissioner (OAIC) was first launched in July 2020. Angelene Falk, Australian Information Commissioner and Privacy Commissioner, commented: “The joint investigation with the ICO has been highly valuable and demonstrates the benefits of data protection regulators collaborating to support effective and proactive regulation.  The issues raised by Clearview AI’s business practices presented novel concerns in a number of jurisdictions. By partnering together, the OAIC and ICO have been able to contribute to an international position, and shape our global regulatory environment.” Falk concluded that uploading an image to a social media site “does not unambiguously indicate agreement to collection of that image by an unknown third party for commercial purposes”. The OAIC ordered Clearview AI to destroy the biometric data it collected of Australians. “People expect that their personal information will be respected, regardless of where in the world their data is being used. That is why global companies need international enforcement. Working with colleagues around the world helped us take this action and protect people from such intrusive activity,” added Edwards. “This international cooperation is essential to protect people’s privacy rights in 2022. That means working with regulators in other countries, as we did in this case with our Australian colleagues. And it means working with regulators in Europe, which is why I am meeting them in Brussels this week so we can collaborate to tackle global privacy harms.” (Photo by quan le on Unsplash) Related: Clearview AI agrees to restrict sales of its faceprint database Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: Australia, clearview ai, ethics, face recognition, facial recognition, ico, Information Commissioner’s Office, investigation, oaic, privacy, probe, Society, uk ","['” falk', 'joint investigation', 'clearview ai', '5 million', 'issues raised', 'controversial facial', 'working colleagues', 'today ’', 'oaic ordered', '“ clearview']"
Zoom receives backlash for emotion-detecting AI,https://www.artificialintelligence-news.com/2022/05/19/zoom-receives-backlash-for-emotion-detecting-ai/,"  Zoom has caused a stir following reports that it’s developing an AI system for detecting emotions. The system, first reported by Protocol, claims to scan users’ faces and their speech to determine their emotions. Zoom detailed the system more in a blog post last month. The company says ‘Zoom IQ’ will be particularly useful for helping salespeople improve their pitches based on the emotions of call participants. Naturally, the system is seen as rather dystopian and has received its fair share of criticism. On Wednesday, over 25 rights groups sent a joint letter to Zoom CEO Eric Yuan. The letter urges Zoom to cease research on emotion-based AI. The letter’s signatories include the American Civil Liberties Union (ACLU), Muslim Justice League, and Access Now. One of the key concerns is that emotion-detecting AI could be used for things like hiring or financial decisions; such as whether to grant loans. That has the possibility to increase existing inequalities. “Results are not intended to be used for employment decisions or other comparable decisions. All recommended ranges for metrics are based on publicly available research,” Zoom explained. Zoom IQ tracks metrics including: Esha Bhandari, Deputy Director of the ACLU Speech, Privacy, and Technology Project, called emotion-detecting AI “creepy” and “a junk science”. (Photo by iyus sugiharto on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here.","['company says', 'zoom caused', '25 rights', 'detecting ai', 'detecting ai', 'claims scan', 'letter ’', '“ results', 'system seen', 'explore upcoming']"
Apple’s former ML director reportedly joins Google DeepMind,https://www.artificialintelligence-news.com/2022/05/18/apple-former-ml-director-reportedly-joins-google-deepmind/,"  A machine learning exec who left Apple due to its return-to-office policy has reportedly joined Google DeepMind.  Ian Goodfellow is a renowned machine learning researcher. Goodfellow invented generative adversarial networks (GANs), developed a system for Google Maps that transcribes addresses from Street View car photos, and more. In a departure note to his team at Apple, Goodfellow cited the company’s much-criticised lack of flexibility in its work policies. Many companies were forced into supporting remote work during the pandemic and many have since decided to keep flexible working due to the recruitment advantages, mental/physical health benefits, lowering the impact of rocketing fuel costs, improved productivity, and reduced office space costs. Apple planned for employees to work from the office on Mondays, Tuesdays, and Thursdays, starting this month. However, following backlash, on Tuesday the company put the plan on hold—officially citing rising Covid cases. Goodfellow already decided to hand in his resignation and head to a company with more forward-looking, modern working policies. The machine learning researcher had worked for Apple since 2019. Prior to Apple, Goodfellow had previously worked for Google as a senior research scientist. Goodfellow is now reportedly returning to Google, albeit to its DeepMind subsidiary. Google is currently approving requests from most employees seeking to work from home. More departures are expected from Apple if it proceeds with its return-to-office mandate. “Everything happened with us working from home all day, and now we have to go back to the office, sit in traffic for two hours, and hire people to take care of kids at home,” a different former Apple employee told Bloomberg. Every talented AI researcher like Goodfellow that leaves Apple is a potential win for Google and other companies. (Photo by Viktor Forgacs on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here.","['many companies', 'every talented', 'tuesday company', 'developed system', 'goodfellow already', 'explore upcoming', 'google currently', 'machine learning', 'machine learning', '“ everything']"
Nuance partners with The Academy to launch The AI Collaborative,https://www.artificialintelligence-news.com/2022/05/13/nuance-partners-the-academy-launch-the-ai-collaborative/,"  Nuance has partnered with The Health Management Academy (The Academy) to launch The AI Collaborative, an industry group focused on advancing healthcare using artificial intelligence and machine learning. Nuance became a household name for creating the speech engine recognition engine behind Siri. In recent years, the company has put a strong focus on AI solutions for healthcare and is now a full-service partner of 77 percent of US hospitals and is trusted by over 500,000 physicians daily. Earlier this year, Microsoft acquired Nuance with the promise of ushering in a “new era of outcomes-based AI”. Microsoft is also active in the healthcare space and its acquisition of Nuance was investigated by regulators over concerns it may reduce competition. Regulators ultimately ended up giving the deal the thumbs up. The EU’s regulator, for example, concluded that Nuance would continue to face stiff competition in the future, the data Microsoft gains would not provide it with an advantage to shut out competitors, and there’d be no ability/incentive to foreclose existing solutions. “Combining the power of Nuance’s deep vertical expertise and proven business outcomes across healthcare, financial services, retail, telecommunications, and other industries with Microsoft’s global cloud ecosystems will enable us to accelerate our innovation and deploy our solutions more quickly, more seamlessly, and at greater scale to solve our customers’ most pressing challenges,” said Mark Benjamin, CEO of Nuance, at the time of Microsoft’s acquisition. Nuance says both companies represent two of the most trusted and innovative technology organisations in the world. As such, it believes Nuance and Microsoft are in a position to foster a community anchored in collaboration with key leading health systems (LHS) executives and experts across the healthcare ecosystem. That community will be The AI Collaborative. “Our members have expressed their desire for a dedicated space to explore AI in healthcare and its enormous potential to improve outcomes and clinical workflow,” said Renee DeSilva, CEO of The Academy. “We are thrilled to expand our partnership with Microsoft and Nuance to introduce The AI Collaborative, a new program at The Academy designed exclusively for clinical and operational executives who lead their organization’s approach to investing in AI as a strategic initiative.” The AI Collaborative will bring together senior leaders from LHS to understand their current and future needs and create the AI and ML innovations required to fulfil them. “The key to successful healthcare innovation using AI is understanding at a deep level the problems that you’re trying to solve and focusing on the outcomes you want to achieve,” explained Peter Durlach, Chief Strategy Officer of Nuance. “With the combined engineering, market and domain expertise of Nuance and Microsoft, The AI Collaborative can bring together multiple technical, business and clinical stakeholders to prioritize deployment of solutions for clinician burnout, patient engagement and health system financial stability, while accelerating innovation in precision medicine, drug discovery, clinical decision support and other promising use cases across the entire healthcare ecosystem.” The AI Collaborative will commence in September 2022 and kick off with a visit to Microsoft’s corporate HQ. Annual summits will be held going forward where stakeholders will learn how to best utilise patient-specific data and insights to augment care delivery, reduce care variation, and support operational improvements. (Photo by Cytonn Photography on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, health, healthcare, machine learning, nuance, the academy, the ai collaborative, the health management academy ","['ai collaborative', 'key successful', 'members expressed', 'academy designed', 'industries microsoft', '“ combining', 'believes nuance', 'microsoft active', 'ai collaborative', 'nuance became']"
Clearview AI agrees to restrict sales of its faceprint database,https://www.artificialintelligence-news.com/2022/05/10/clearview-ai-agrees-restrict-sales-faceprint-database/,"  Clearview AI has proposed to restrict sales of its faceprint database as part of a settlement with the American Civil Liberties Union (ACLU). The controversial facial recognition firm caused a stir due to scraping billions of images of people across the web without their consent. As a result, the company has faced the ire of regulators around the world and numerous court cases. One court case filed against Clearview AI was by the ACLU in 2020, claiming that it violated the Biometric Information Privacy Act (BIPA). That act covers Illinois and requires companies operating in the state to obtain explicit consent from individuals to collect their biometric data. “Fourteen years ago, the ACLU of Illinois led the effort to enact BIPA – a groundbreaking statute to deal with the growing use of sensitive biometric information without any notice and without meaningful consent,” explained Rebecca Glenberg, staff attorney for the ACLU of Illinois. “BIPA was intended to curb exactly the kind of broad-based surveillance that Clearview’s app enables.” The case is ongoing but the two sides have reached a draft settlement. As part of the proposal, Clearview AI has agreed to restrict sales of its faceprint database to businesses and other private entities across the country. “By requiring Clearview to comply with Illinois’ pathbreaking biometric privacy law not just in the state, but across the country, this settlement demonstrates that strong privacy laws can provide real protections against abuse,” said Nathan Freed Wessler, a deputy director of the ACLU Speech, Privacy, and Technology Project. “Clearview can no longer treat people’s unique biometric identifiers as an unrestricted source of profit. Other companies would be wise to take note, and other states should follow Illinois’ lead in enacting strong biometric privacy laws.”  The most protections will be offered to residents in Illinois. Clearview AI will be banned from sharing access to its database to any private company in the state in addition to any local public entity for five years. Furthermore, Clearview AI plans to filter out images from Illinois. This may not catch all images so residents will be able to upload their image and Clearview will block its software from finding matches for their face. Clearview AI will spend $50,000 on adverts in online ads to raise awareness for this feature. “This settlement is a big win for the most vulnerable people in Illinois,” commented Linda Xóchitl Tortolero, president and CEO of Mujeres Latinas en Acción, a Chicago-based non-profit. “Much of our work centres on protecting privacy and ensuring the safety of survivors of domestic violence and sexual assault. Before this agreement, Clearview ignored the fact that biometric information can be misused to create dangerous situations and threats to their lives. Today that’s no longer the case.”  The protections offered to American citizens outside Illinois aren’t quite as stringent. Clearview AI is still able to sell access to its huge database to public entities, including law enforcement. In the wake of the US Capitol raid, the company boasted that police use of its facial recognition system increased 26 percent. However, the company would be banned from selling access to its complete database to private companies. Clearview AI could still sell its software, but any purchaser would need to source their own database to train it. “There is a battle being fought in courtrooms and statehouses across the country about who is going to control biometrics—Big Tech or the people being tracked by them—and this represents one of the biggest victories for consumers to date,” said J. Eli Wade-Scott from Edelson PC. In November 2021, the UK’s Information Commissioner’s Office (ICO) imposed a potential fine of just over £17 million to Clearview AI and ordered the company to destroy the personal data it holds on British citizens and cease further processing. Earlier that month, the OAIC reached the same conclusion as the ICO and ordered Clearview AI to destroy the biometric data it collected on Australians and cease further collection. The full draft settlement between Clearview AI and the ACLU can be found here. (Photo by Maksim Chernishev on Unsplash) Related: Ukraine harnesses Clearview AI to uncover assailants and identify the fallen Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: aclu, american civil liberties union, biometric information privacy act, bipa, clearview ai, ethics, face recognition, facial recognition, law, legal, Legislation, privacy, Society, surveillance ","['aclu illinois', 'imposed potential', 'controversial facial', 'ukraine harnesses', 'clearview ai', 'oaic reached', 'act covers', 'battle fought', '“ much', 'clearview ai']"
What leveraging AI in hybrid security systems means for enterprises,https://www.artificialintelligence-news.com/2022/05/10/what-leveraging-ai-in-hybrid-security-systems-means-for-enterprises/,"  Artificial intelligence (AI) is becoming more common than you may realise. Many of society’s leading technologies are driven by AI technology, as their automated functions streamline processes and help people do more with less time. Now, AI is integrating into commercial security systems and starting to revolutionise technology. Modern security systems with AI technology can help security teams better detect threats and provide faster responses to protect your business more effectively.  Enterprises can leverage AI to enable security operators to analyse data more efficiently and streamline operations, allowing teams to adjust their focuses to more critical matters and better detect anomalies as they occur. Altogether, AI empowers your security teams to provide better and faster responses to threats, strengthening your security systems for the safety of your enterprise.  One use case for AI is leveraging its learning capabilities to automate responses. AI can be used to evaluate patterns of data over time, and learn from it. By formulating automated responses, AI streamlines necessary processes, allowing security teams to focus on the most critical matters. In many cases, AI empowers users to perform necessary tasks more efficiently, while maintaining the data safety and organisational standards required for optimal operations.  When converging physical and cybersecurity systems, AI technology is useful for analysing combined data streams. Learned behaviours can make managing the millions of data points coming from across an enterprise network of systems more streamlined, helping security teams pinpoint areas of concern with automated alerts, as well as facilitating efficient audits for security trends over time. For example, if your security team repeatedly dismisses a specific alert on their video security system, over time a pattern will form that AI technology will recognise. It can trigger an automated response to dismiss this alert, reducing the number of unnecessary alerts. AI interprets data and uses it to inform its responses, streamlining your system effectively. However, it’s important that your system maintains a record of all alerts and activity so the system can be audited regularly to ensure optimal functionality.  AI’s automated responses and workflows can substantially impact your converged security system’s productivity and accuracy. With workforces adopting more hybrid schedules, there is a need for security teams to be increasingly flexible and available. AI can help cyber and physical security teams be more agile and efficient even as more data and information comes their way. This reduces unnecessary burdens on your converged security team, allowing them to move their focus onto more critical matters and complete work productively.  Take a look at how the Openpath Video Intercom Reader Pro leverages AI to facilitate visitor access. When a visitor, delivery courier, or vendor initiates a call using the doorbell on the reader, the intelligent voice system routes the call to the correct person based on the responses from the guest. The system can even be programmed to route calls to secondary teams or a voicemail service based on tenant availability and door schedules.  With access control, video security, and cybersecurity systems, AI can be used to help security operators determine which areas need immediate focus, provide real-time alerts, and help security teams increase their productivity to ensure that your enterprise remains safe and performs to the best of its ability.  A good example of using AI to strengthen commercial security systems is detecting anomalies in the security network and behaviours. Especially in large enterprises, it can be difficult for security staff to monitor every single instance across the network, so data-driven AI learns to recognise specific changes or patterns. These anomalies may come in the form of user behaviours, data packages sent over the network, or hacking attempts on cybersecurity systems.  AI can detect abnormal network behaviour using a baseline of what is common and what isn’t. For example, Ava Aware uses AI in their video security software to alert security staff to detect unusual motion or behaviour. If the AI does notice an anomaly, an automated response alerts security staff to the threat, allowing them to evaluate and take appropriate action. Remote access and real-time notifications help keep your on-prem and cloud-based security systems safe even when your security team is away from the office.  While AI is helpful in detecting anomalies to common patterns and attacks, it’s not fool proof. Sophisticated attacks can hide their signature and trick AI systems into ignoring the threat. Human monitoring and intervention is still necessary, and you should never depend solely on AI to protect your security systems. Overall, AI can assist your team in detecting threats and anomalies across your security system on a large scale, and allow security teams to act proactively and productively to protect your enterprise.  Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: AI enterprise, ai security, security ","['modern security', 'ava aware', 'system even', '’ important', 'learned behaviours', 'ai help', 'ai ’', 'help security', 'good example', 'take look']"
Kendrick Lamar uses deepfakes in latest music video,https://www.artificialintelligence-news.com/2022/05/09/kendrick-lamar-uses-deepfakes-in-latest-music-video/,"  American rapper Kendrick Lamar has made use of deepfakes for his latest music video. Deepfakes use generative neural network architectures –  such as autoencoders or generative adversarial networks (GANs) – to manipulate or generate visual and audio content. Lamar is widely considered one of the greatest rappers of all time. However, he’s regularly proved his creative mind isn’t limited to his rapping talent. For his track ‘The Heart Part 5’, Lamar has made use of deepfake technology to seamlessly morph his face into various celebrities including Kanye West, Nipsey Hussle, Will Smith, and even O.J. Simpson. You can view the music video below: For due credit, the deepfake element was created by a studio called Deep Voodoo. Deepfakes are often used for entertainment purposes, including for films and satire. However, they’re also being used for nefarious purposes like the creation of ‘deep porn’ videos without the consent of those portrayed. The ability to deceive has experts concerned about the social implications. Deepfakes could be used for fraud, misinformation, influencing public opinion, and interfering in democratic processes. In March, a deepfake purportedly showing Ukrainian President Volodymyr Zelenskyy asking troops to lay down their arms in their fight to defend their homeland from Russia’s invasion was posted to a hacked news website. “I only advise that the troops of the Russian Federation lay down their arms and return home,” Zelenskyy said in an official video to refute the fake. “We are at home and defending Ukraine.” Fortunately, the deepfake was of very low quality by today’s standards. The fake Zelenskyy had a comically large and noticeably pixelated head compared to the rest of his body. The video probably didn’t fool anyone, but it could have had major consequences if people did believe it. One Russia-linked influence campaign – removed by Facebook and Twitter in March – used AI-generated faces for a fake “editor-in-chief” and “columnist” for a linked propaganda website. The more deepfakes that are exposed will increase public awareness. Artists like Kendrick Lamar using them for entertainment purposes will also help to spread awareness that you can no longer necessarily believe what you can see with your own eyes. Related: Humans struggle to distinguish between real and AI-generated faces Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California, and London. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, deepfakes, Entertainment, generative adversarial networks, kendrick lamar, music video, neural network, the heart part 5 ","['deepfake purportedly', 'artists like', '’ used', 'lamar made', 'linked influence', 'fake zelenskyy', 'deepfakes use', 'american rapper', '’ regularly', 'advise troops']"
Editorial: Our predictions for the AI industry in 2022,https://www.artificialintelligence-news.com/2021/12/23/editorial-our-predictions-for-the-ai-industry-in-2022/,"  The AI industry continued to thrive this year as companies sought ways to support business continuity through rapidly-changing situations. For those already invested, many are now doubling-down after reaping the benefits. As we wrap up the year, it’s time to look ahead at what to expect from the AI industry in 2022. Tackling bias Our ‘Ethics & Society’ category got more use than most others this year, and with good reason. AI cannot thrive when it’s not trusted. Biases are present in algorithms that are already causing harm. They’ve been the subject of many headlines, including a number of ours, and must be addressed for the public to have confidence in wider adoption. Explainable AI (XAI) is a partial solution to the problem. XAI is artificial intelligence in which the results of the solution can be understood by humans. Robert Penman, Associate Analyst at GlobalData, comments: “2022 will see the further rollout of XAI, enabling companies to identify potential discrimination in their systems’ algorithms. It is essential that companies correct their models to mitigate bias in data. Organisations that drag their feet will face increasing scrutiny as AI continues to permeate our society, and people demand greater transparency. For example, in the Netherlands, the government’s use of AI to identify welfare fraud was found to violate European human rights. Reducing human bias present in training datasets is a huge challenge in XAI implementation. Even tech giant Amazon had to scrap its in-development hiring tool because it was claimed to be biased against women. Further, companies will be desperate to improve their XAI capabilities—the potential to avoid a PR disaster is reason enough.” To that end, expect a large number of acquisitions of startups specialising in synthetic data training in 2022. Smoother integration Many companies don’t know how to get started on their AI journeys. Around 30 percent of enterprises plan to incorporate AI into their company within the next few years, but 91 percent foresee significant barriers and roadblocks. If the confusion and anxiety that surrounds AI can be tackled, it will lead to much greater adoption. Dr Max Versace, PhD, CEO and Co-Founder of Neurala, explains: “Similar to what happened with the introduction of WordPress for websites in early 2000, platforms that resemble a ‘WordPress for AI’ will simplify building and maintaining AI models.  In manufacturing for example, AI platforms will provide integration hooks, hardware flexibility, ease of use by non-experts, the ability to work with little data, and, crucially, a low-cost entry point to make this technology viable for a broad set of customers.” AutoML platforms will thrive in 2022 and beyond. From the cloud to the edge The migration of AI from the cloud to the edge will accelerate in 2022. Edge processing has a plethora of benefits over relying on cloud servers including speed, reliability, privacy, and lower costs. Versace commented: “Increasingly, companies are realising that the way to build a truly efficient AI algorithm is to train it on their own unique data, which might vary substantially over time. To do that effectively, the intelligence needs to directly interface with the sensors producing the data.  From there, AI should run at a compute edge, and interface with cloud infrastructure only occasionally for backups and/or increased functionality. No critical process – for example,  in a manufacturing plant – should exclusively rely on cloud AI, exposing the manufacturing floor to connectivity/latency issues that could disrupt production.” Expect more companies to realise the benefits of migrating from cloud to edge AI in 2022. Doing more with less Among the early concerns about the AI industry is that it would be dominated by “big tech” due to the gargantuan amount of data they’ve collected. However, innovative methods are now allowing algorithms to be trained with less information. Training using smaller but more unique datasets for each deployment could prove to be more effective. We predict more startups will prove the world doesn’t have to rely on big tech in 2022. Human-powered AI While XAI systems will provide results which can be understood by humans, the decisions made by AIs will be more useful because they’ll be human-powered. Varun Ganapathi, PhD, Co-Founder and CTO at AKASA, said: “For AI to truly be useful and effective, a human has to be present to help push the work to the finish line. Without guidance, AI can’t be expected to succeed and achieve optimal productivity. This is a trend that will only continue to increase. Ultimately, people will have machines report to them. In this world, humans will be the managers of staff – both other humans and AIs – that will need to be taught and trained to be able to do the tasks they’re needed to do. Just like people, AI needs to constantly be learning to improve performance.” Greater human input also helps to build wider trust in AI. Involving humans helps to counter narratives about AI replacing jobs and concerns that decisions about people’s lives could be made without human qualities such as empathy and compassion. Expect human input to lead to more useful AI decisions in 2022. Avoiding captivity The telecoms industry is currently pursuing an innovation called Open RAN which aims to help operators avoid being locked to specific vendors and help smaller competitors disrupt the relative monopoly held by a small number companies. Enterprises are looking to avoid being held in captivity by any AI vendor. Doug Gilbert, CIO and Chief Digital Officer at Sutherland, explains: “Early adopters of rudimentary enterprise AI embedded in ERP / CRM platforms are starting to feel trapped. In 2022, we’ll see organisations take steps to avoid AI lock-in. And for good reason. AI is extraordinarily complex. When embedded in, say, an ERP system, control, transparency, and innovation is handed over to the vendor not the enterprise. AI shouldn’t be treated as a product or feature: it’s a set of capabilities. AI is also evolving rapidly, with new AI capabilities and continuously improved methods of training algorithms. To get the most powerful results from AI, more enterprises will move toward a model of combining different AI capabilities to solve unique problems or achieve an outcome. That means they’ll be looking to spin up more advanced and customizable options and either deprioritising AI features in their enterprise platforms or winding down those expensive but basic AI features altogether.” In 2022 and beyond, we predict enterprises will favour AI solutions that avoid lock-in. Chatbots get smart Hands up if you’ve ever screamed (internally or externally) that you just want to speak to a human when dealing with a chatbot—I certainly have, more often than I’d care to admit. “Today’s chatbots have proven beneficial but have very limited capabilities. Natural language processing will start to be overtaken by neural voice software that provides near real time natural language understanding (NLU),” commented Gilbert. “With the ability to achieve comprehensive understanding of more complex sentence structures, even emotional states, break down conversations into meaningful content, quickly perform keyword detection and named entity recognition, NLU will dramatically improve the accuracy and the experience of conversational AI.” In theory, this will have two results: In 2022, chatbots will get much closer to offering a human-like experience. It’s not about size, it’s about the quality A robust AI system requires two things: a functioning model and underlying data to train that model. Collecting huge amounts of data is a waste of time if it’s not of high quality and labeled correctly. Gabriel Straub, Chief Data Scientist at Ocado Technology, said: “Andrew Ng has been speaking about data-centric AI, about how improving the quality of your data can often lead to better outcomes than improving your algorithms (at least for the same amount of effort.) So, how do you do this in practice? How do you make sure that you manage the quality of data at least as carefully as the quantity of data you collect? There are two things that will make a big difference: 1) making sure that data consumers are always at the heart of your data thinking and 2) ensuring that data governance is a function that enables you to unlock the value in your data, safely, rather than one that focuses on locking down data.” Expect the AI industry to make the quality of data a priority in 2022. (Photo by Michael Dziedzic on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Tags: 2022, ai, ai predictions, artificial intelligence, automl, bias, chatbots, datasets, edge ai, edge computing, enterprise, ethics, explainable ai, platforms, predictions, virtual assistants, xai ","['telecoms industry', 'involving humans', 'means ’', 'less among', 'natural language', 'humans managers', 'enterprises move', 'government ’', 'ai industry', 'around 30']"
The EU’s AI rules will likely take over a year to be agreed,https://www.artificialintelligence-news.com/2022/02/17/eu-ai-rules-likely-take-over-year-to-be-agreed/,"  Rules governing the use of artificial intelligence across the EU will likely take over a year to be agreed upon. Last year, the European Commission drafted AI laws. While the US and China are set to dominate AI development with their vast resources, economic might, and light-touch regulation, European rivals – including the UK and EU members – believe they can lead in ethical standards. In the draft of the EU regulations, companies that are found guilty of AI misuse face a fine of €30 million or six percent of their global turnover (whichever is greater). The risk of such fines has been criticised as driving investments away from Europe. The EU’s draft AI regulation classifies systems into three risk categories: Unacceptable risk systems will face a blanket ban from deployment in the EU while limited risk will require minimal oversight. Organisations deploying high-risk AI systems would be required to have things like: However, the cumbersome nature of the EU – requiring agreement from all member states, each with their own priorities – means that new regulations are often subject to more debate and delay than national lawmaking. Reuters reports that two key lawmakers on Wednesday said the EU’s AI regulations will likely take over a year more to agree. The delay is primarily due to debates over whether facial recognition should be banned and who should enforce the rules. “Facial recognition is going to be the biggest ideological discussion between the right and left,” said one lawmaker, Dragos Tudorache, in a Reuters interview. “I don’t believe in an outright ban. For me, the solution is to put the right rules in place.” With leading academic institutions and more than 1,300 AI companies employing over 30,000 people, the UK is the biggest destination for AI investment in Europe and the third in the world. Between January and June 2021, global investors poured £13.5 billion into more than 1,400 “deep tech” UK private technology firms—more than Germany, France, and Israel combined. In September 2021, the UK published its 10-year National Artificial Intelligence Strategy in a bid to secure its European AI leadership. Governance plays a large role in the strategy. “The UK already punches above its weight internationally and we are ranked third in the world behind the USA and China in the list of top countries for AI,” commented DCMS Minister Chris Philp. “We’re laying the foundations for the next ten years’ growth with a strategy to help us seize the potential of artificial intelligence and play a leading role in shaping the way the world governs it.” As part of its strategy, the UK is creating an ‘AI Standards Hub’ to coordinate the country’s engagement in establishing global rules and is working with The Alan Turing Institute to update guidance on AI ethics and safety. “We are proud of creating a dynamic, collaborative community of diverse researchers and are growing world-leading capabilities in responsible, safe, ethical, and inclusive AI research and innovation,” said Professor Sir Adrian Smith, Chief Executive of The Alan Turing Institute. Striking a balance between innovation-stifling overregulation and ethics-compromising underregulation is never a simple task. It will be interesting to observe how AI regulations in Europe will differ across the continent and beyond. (Photo by Christian Lue on Unsplash) Related: British intelligence agency GCHQ publishes ‘Ethics of AI’ report Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, draft, ethics, eu, europe, government, laws, legal, Legislation, Politics, regulations, rules, Society, strategy, uk ","['’ laying', 'british intelligence', 'uk already', 'uk creating', 'reuters reports', 'companies found', 'unacceptable risk', 'european rivals', 'rules governing', 'priorities –']"
Babylon Health taps Google Cloud to boost scalability and innovation￼,https://www.artificialintelligence-news.com/2022/03/28/babylon-health-google-cloud-boost-scalability-innovation/,"  AI-powered healthcare service Babylon Health has announced a partnership with Google Cloud to boost scalability and innovation. London-based Babylon Health is a digital-first health service provider that uses AI and machine learning technology to provide access to health information to people whenever and wherever they need it. The company has partnered with private and public across the UK, North America, South-East Asia, and Rwanda with the aim of making healthcare more accessible and affordable to 24 million patients worldwide. “Our job is to help people to stay well and we’re on a mission to provide affordable, accessible health care to everyone in the world,” explains Richard Noble, Engineering Director of Data at Babylon. Babylon Health’s rapid growth has led it to seek a partner to help it scale. By partnering with Google Cloud, the company claims that it’s been able to: Babylon Health needs to store and process huge amounts of sensitive data. “We work with a lot of private patient data and we must ensure that it stays private,” explains Natalie Godec, cloud engineer at Babylon. “At the same time, we must enable our teams to innovate with that data while meeting different national regulatory standards.” Therefore, Babylon Health required a partner it felt could handle such demands. “We chose Google Cloud because we knew it could scale with us and support us with our data science and analysis and we could build the tools we needed with it quickly,” added Noble. “It offers the solutions that enable us to focus on our core business, access to health.” Babylon Health says the move to Google Cloud has enabled it to better analyse its data using AI to unlock new tools and features that help clinicians and users alike. While building a new data model and giving access to users initially took six months, the company says it now takes under a week. In London, Babylon Health offers its ‘GP at Hand’ service which – in partnership with the NHS – acts as a digital GP practice. Patients can connect to NHS clinicians remotely 24/7 and even be issued prescriptions if required. Where physical examinations are needed, patients will be directed to a suitable venue. However, GP at Hand has been criticised as “cherry-picking” healthier patients—taking resources away from local GP practices that are often trying to care for sicker, more elderly patients. While initial problems are to be expected from any relatively new service; poor advice in a healthcare service could result in unnecessary suffering, long-term complications, or even death. In 2018, Dr David Watkins – a consultant oncologist at Royal Marsden Hospital – reached out to AI News to alert us to Babylon Health’s chatbot giving unsafe advice. Dr Watkins provided numerous examples of clearly dangerous advice being given by the chatbot: Babylon Health called Dr Watkins a “troll” who has “targeted members of our staff, partners, clients, regulators and journalists and tweeted defamatory content about us”. According to Babylon Health, Dr Watkins conducted 2,400 tests of the chatbot in a bid to discredit the service while raising “fewer than 100 test results which he considered concerning”. Babylon Health claims that in just 20 cases did Dr Watkins find genuine errors while others were “misrepresentations” or “mistakes,” according to Babylon’s own “panel of senior clinicians” who remain unnamed. Dr Watkins called Babylon’s claims “utterly nonsense” and questions where the startup got its figures from as “there are certainly not 2,400 completed triage assessments”. He estimates conducting between 800 and 900 full triages and that some were repeat tests to see whether Babylon Health had fixed the issues he previously highlighted. That same year, Babylon Health published a paper claiming that its AI could diagnose common diseases as well as human physicians. The Royal College of General Practitioners, the British Medical Association, Fraser and Wong, and the Royal College of Physicians all issued statements disputing the paper’s claims. Dr Watkins has acknowledged that Babylon Health’s chatbot has improved and has substantially reduced its error rate. In 2018, when Dr Watkins first reached out to us, he says this rate was “one in one”. In 2020, Babylon Health claimed in a paper that it can now appropriately triage patients in 85 percent of cases. Hopefully, the partnership with Google Cloud continues to improve Babylon Health’s abilities to help it achieve its potentially groundbreaking aim to deliver 24/7 access to healthcare wherever a patient is. (Photo by Hush Naidoo Jade Photography on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, babylon health, big data, cloud, google cloud, gp at hand, health, healthcare, machine learning ","['dr david', '” babylon', 'estimates conducting', 'chose google', 'dr watkins', 'partnership google', 'first health', 'babylon health', '400 tests', 'babylon health']"
"SAS: Finland leads in hyperautomation awareness, UK languishing",https://www.artificialintelligence-news.com/2022/04/12/sas-finland-leads-hyperautomation-awareness-uk-languishing/,"  Research from analytics firm SAS has found that Finland appears to be leading in hyperautomation awareness while a number of large countries are falling behind. Hyperautomation brings together AI, robotic process automation, and multiple business processes on a cloud platform to help organisations make better decisions more quickly. “Organisations will require more IT and business process automation as they are forced to accelerate digital transformation plans in a post-COVID-19, digital-first world,” said Fabrizio Biscotti, Research Vice President at Gartner. Businesses that adopt hyperautomation as part of their digital transformation journeys will gain a competitive edge over rivals. “We know from our own research that companies are already using hyperautomation to drive efficiency, customer service and innovation,” commented David Shannon, Head of Hyperautomation at SAS UK & Ireland. However, it seems organisations in some countries are exploring how it could help their business more than others. For its research, SAS analysed online search data for 24 countries to determine awareness of hyperautomation and where demand seems to be growing fastest. Here are the top 10 countries for hyperautomation awareness: The UK is in 12th place with just 1.25 people in every 10,000 searching for hyperautomation. However, that’s still ahead of countries including the US, France, Spain, Italy, and Japan, which were all in the bottom 10 out of the 24 countries analysed by SAS. Spain and Italy at least make appearances in the list of the 10 countries where search has grown fastest—meaning they’re best placed to push their way into the top 10 for awareness before long. Here are the 10 countries where search has grown fastest: With the UK all the way down in 18th place on the list with an increase of 20.4 percent, it’s clear that far more awareness is required for the country to have much chance of becoming a hyperautomation leader. “While search data can’t tell us everything, it’s a useful barometer to test the mood of nations. Awareness has grown rapidly in some countries in just three years, indicating where demand could be strongest,” added Shannon. “Automation has enabled businesses of all sizes to transform their operations, and hyperautomation is the next step in the journey. Now is the time for them to build it, and other advanced technological solutions, into their digital strategies.” (Photo by Josh Redd on Unsplash) Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo taking place in Amsterdam, California and London throughout 2022. Explore other upcoming enterprise technology events and webinars powered by TechForge here. Tags: ai, artificial intelligence, automation, enterprise, hyperautomation, report, research, sas, study ","['spain italy', 'research analytics', 'sas analysed', 'multiple business', '’ clear', 'businesses adopt', 'business process', 'seems organisations', 'know research', 'uk way']"
